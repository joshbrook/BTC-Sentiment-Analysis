{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8031b7fc",
   "metadata": {},
   "source": [
    "# Bitcoin Prediction based on Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed56d7b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For our project, we have chosen to use twitter data to perform a sentiment analysis on users opinions about crypto currencies inorder to create a predictive model that relies on the sentiment to predict how different crpyto currencies may behave.\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "First, we selected the datasets we would use for the analysis as well as to train and test our models later on.\n",
    "We have chosen the following datasets for our analysis:\n",
    "- Covid-19 Twitter chatter dataset\n",
    "  > The data can be obtained from the following repository directly from the publisher however a copy is included as part of our project which is within the stiupulated terms of use by the publisher of the data as well as Twitter:https://github.com/thepanacealab/covid19_twitter\n",
    "- Apple Twitter dataset\n",
    "  > This dataset can be obtained from: https://socialgrep.com/datasets/five-years-of-aapl-on-reddit\n",
    "- Ucc(The Unhealthy Comments Corpus)\n",
    "  > This dataset can be obtained from: https://github.com/conversationai/unhealthy-conversations\n",
    "\n",
    "### Research Question \n",
    "We chose to investigate how the price of Bitcoin may be affected by twitter sentiments about the currency based on a sentiment analysis model trained on the UCC corpus and a final prediction model based on the sentiment model.\n",
    "\n",
    "\n",
    "To begin, we must first install some modules required to access the data, as per the publisher (Banda et al., 2021):\n",
    "- Twarc\n",
    "- Tweepy (v. 3.8.0)\n",
    "- Argparse (v 3.2)\n",
    "- Xtract (v 0.1 a3)\n",
    "- Wget (v 3.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150a1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install twarc \n",
    "!pip install tweepy \n",
    "!pip install argparse \n",
    "!pip install xtract \n",
    "!pip install ipywidgets\n",
    "!pip install wget\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb4020",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before we run the analysis, we need to filter and process our data to extract only the English tweets.\n",
    "To acces the data, I use the instructions provided by Banda et al from the website: https://github.com/thepanacealab/covid19_twitter/blob/master/COVID_19_dataset_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141a1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import csv\n",
    "import wget\n",
    "import linecache\n",
    "from shutil import copyfile\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfc59c",
   "metadata": {},
   "source": [
    "## Filtering dataset by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed88865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f881c91d68e44868a91071f912ad99c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=14, options=('all', 'am', 'ar', 'bg', 'bn', 'bo', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'eâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unzips the dataset and gets the TSV dataset\n",
    "with gzip.open('clean-dataset.tsv.gz', 'rb') as f_in:\n",
    "    with open('clean-dataset.tsv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "\n",
    "#Gets all possible languages from the dataset\n",
    "df = pd.read_csv('clean-dataset.tsv',sep=\"\\t\")\n",
    "lang_list = df.lang.unique()\n",
    "lang_list= sorted(np.append(lang_list,'all'))\n",
    "lang_picker = widgets.Dropdown(options=lang_list, value=\"en\")\n",
    "lang_picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7731e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShowing first 5 tweets from the filtered dataset\u001b[0m\n",
      "['1351757442653294592\\t2021-01-20\\t05:04:23\\ten\\tNULL\\n', '1351757444033069056\\t2021-01-20\\t05:04:23\\ten\\tNULL\\n', '1351757446860083202\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n', '1351757447619375106\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n', '1351757448219140105\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n']\n"
     ]
    }
   ],
   "source": [
    "#Creates a new clean dataset with the english tweets and prints out the first 5 twits from the filtered dataset.\n",
    "filtered_language = lang_picker.value\n",
    "\n",
    "#If no language specified, it will get all records from the dataset\n",
    "if filtered_language == \"\":\n",
    "    copyfile('clean-dataset.tsv', 'clean-dataset-filtered.tsv')\n",
    "\n",
    "#If language specified, it will create another tsv file with the filtered records\n",
    "else:\n",
    "    filtered_tw = list()\n",
    "    current_line = 1\n",
    "    with open(\"clean-dataset.tsv\") as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        if current_line == 1:\n",
    "            filtered_tw.append(linecache.getline(\"clean-dataset.tsv\", current_line))\n",
    "\n",
    "            for line in tsvreader:\n",
    "                if line[3] == filtered_language:\n",
    "                      filtered_tw.append(linecache.getline(\"clean-dataset.tsv\", current_line))\n",
    "                current_line += 1\n",
    "\n",
    "print('\\033[1mShowing first 5 tweets from the filtered dataset\\033[0m')\n",
    "print(filtered_tw[1:(6 if len(filtered_tw) > 6 else len(filtered_tw))])\n",
    "\n",
    "with open('clean-dataset-filtered.tsv', 'w') as f_output:\n",
    "    for item in filtered_tw:\n",
    "        f_output.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0d256",
   "metadata": {},
   "source": [
    "## Authentication \n",
    "Accessing the Twitter APIs requires a set of credentials that you must pass with each request. These credentials can come in different forms depending on the type of authentication that is required by the specific endpoint that you are using. More information: https://developer.twitter.com/en/docs/apps/overview\n",
    "\n",
    "The credentials can be obtained from the developer portal (https://developer.twitter.com/en/portal/dashboard).\n",
    "**Ensure that you have an elevated account as the basic essential credentials will not allow you to access the data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c138bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Authenticate\n",
    "CONSUMER_KEY = \"jqDKBsklkmmcEWpurFa6NoeHu\" #@param {type:\"string\"}\n",
    "CONSUMER_SECRET_KEY = \"cvMgSe1FhVuzJrwukiFG0UgTugvfOdRjdZKMTOWCjOQY4mqkLD\" #@param {type:\"string\"}\n",
    "ACCESS_TOKEN_KEY = \"1069349234-lyOIiqDVKaCkYNBaoeS0cxoNcn8VZjezWflZ8Xn\" #@param {type:\"string\"}\n",
    "ACCESS_TOKEN_SECRET_KEY = \"N9ClbBxTiUlcTY0vB0j5way3vzjWrLoaS0BQ7K46NBhWO\" #@param {type:\"string\"}\n",
    "\n",
    "#Creates a JSON Files with the API credentials\n",
    "with open('api_keys.json', 'w') as outfile:\n",
    "    json.dump({\n",
    "    \"consumer_key\":CONSUMER_KEY,\n",
    "    \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
    "    \"access_token\":ACCESS_TOKEN_KEY,\n",
    "    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY\n",
    "     }, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cc27c",
   "metadata": {},
   "source": [
    "## Hydrating the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596337fa",
   "metadata": {},
   "source": [
    "Before parsing the dataset, a hydration process is required.It is done by using the following social media mining tool: https://github.com/thepanacealab/SMMT\n",
    "\n",
    "To perform this action, a python file from that repository is required (get_metadata.py).\n",
    "Once obtained, this utility will take a file which meets the following requirements:\n",
    "\n",
    "- A csv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
    "- A text file which contains one tweet id per line\n",
    "- A tsv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
    "- For this case, the filtered dataset generated before (clean-dataset-filtered.tsv), which is in TSV format will be used for the hydration process\n",
    "\n",
    "The arguments for this utily (get_metadata.py) are the following:\n",
    "- i :input file name\n",
    "- 0 :output file name\n",
    "- k :key file name #json file containing your Api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_metadata.py -i clean-dataset-filtered.tsv -o hydrated_tweets -k api_keys.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb437ed",
   "metadata": {},
   "source": [
    "From the code above, the output will generate four files:\n",
    "\n",
    "- A hydrated_tweets.json file which contains the full json object for each of the hydrated tweets\n",
    "- A hydrated_tweets.CSV file which contains partial fields extracted from the tweets.\n",
    "- A hydrated_tweets.zip file which contains a zipped version of the tweets_full.json file.\n",
    "- A hydrated_tweets_short.json which contains a shortened version of the hydrated tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208c69a",
   "metadata": {},
   "source": [
    "## Parsing Tweets\n",
    "Now to Parse the Tweets we need the following files from the data processing tools:\n",
    "- https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/parse_json_lite.py\n",
    "- https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/fields.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install emot --upgrade\n",
    "!pip install emoji --upgrade\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67396e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"hydrated_tweets_short.json\", \"r\") as myfile:\n",
    "    list_tweets = list(myfile)\n",
    "    \n",
    "file = open(\"sample_data.json\", \"w\")\n",
    "for i in list_tweets:\n",
    "    file.write(i)\n",
    "file.close() #This close() is important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e196c",
   "metadata": {},
   "source": [
    "### The following code uses the utility above to parse the data and preprocess it.\n",
    "\n",
    "parse_json_lite.py: The first argument is the json file. The second argument is optional. If the second argument is given, it will preprocess the json file. The preprocessing includes removal of URLs, twitter specific Urls, Emojis, Emoticons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python parse_json_lite.py sample_data.json p\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8f8ed",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "Now that the data is Hydrated and organised in the four files, we can begin the preprocessing of the data.\n",
    "\n",
    "> For our project, we perform a sentiment analysis on tweets related to crypto currencies and use this analysis to predict how the currencies will varry depending on the sentiment. \n",
    "\n",
    "> Since we are only interested in tweets that are related to Bitcoin, we will specify a filter then filter out the tweets that do not contain the words in the filter.\n",
    "\n",
    ">After that, we perform a sentiment analysis using pre trained models to see whether we can accurately predict what the sentiment of the tweets are.\n",
    "\n",
    ">The models used will be trained on the UCC(The Unhealthy Comments Corpus) Coprus that was mentioned before , which contains over 40,000 online comments which have been tagged with sentiment values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2df9d7",
   "metadata": {},
   "source": [
    "## Apple-Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucc_train = pd.read_csv(\"UCC/train.csv\")\n",
    "aapl = pd.read_csv(\"data/Apple-Twitter-Sent.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = aapl.drop(columns=\"_golden _unit_state _last_judgment_at date id query sentiment_gold\".split())\n",
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "tweets = list(zip(aapl[\"text\"], aapl[\"sentiment\"]))\n",
    "\n",
    "ttk = TweetTokenizer()\n",
    "\n",
    "tokens = [(ttk.tokenize(tweet), sentiment) for tweet, sentiment in tweets]\n",
    "\n",
    "filtered = []\n",
    "for tweet in tokens:\n",
    "    toks = []\n",
    "    for tok in tweet[0]:\n",
    "        if tok.isalpha():\n",
    "            toks.append(tok)\n",
    "        if \"#\" in tok or \"@\" in tok:\n",
    "            toks.append(tok)\n",
    "    filtered.append((toks, tweet[1]))\n",
    "\n",
    "tagged = [(nltk.pos_tag(tweet), sentiment) for tweet, sentiment in filtered]\n",
    "\n",
    "tagged[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
