{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8031b7fc",
   "metadata": {},
   "source": [
    "# Bitcoin Prediction based on Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed56d7b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For our project, we have chosen to use twitter data to perform a sentiment analysis on users opinions about crypto currencies inorder to create a predictive model that relies on the sentiment to predict how different crpyto currencies may behave.\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "### Data\n",
    "First, we selected the datasets we would use for the analysis as well as to train and test our models later on.\n",
    "We have chosen the following datasets for our analysis:\n",
    "- Covid-19 Twitter chatter dataset\n",
    "  > The data can be obtained from the following repository directly from the publisher however a copy is included as part of our project which is within the stiupulated terms of use by the publisher of the data as well as Twitter:https://github.com/thepanacealab/covid19_twitter\n",
    "- Apple Twitter dataset\n",
    "  > This dataset can be obtained from: https://socialgrep.com/datasets/five-years-of-aapl-on-reddit\n",
    "- Ucc(The Unhealthy Comments Corpus)\n",
    "  > This dataset can be obtained from: https://github.com/conversationai/unhealthy-conversations\n",
    "\n",
    "### Research Question \n",
    "We chose to investigate how the price of Bitcoin may be affected by twitter sentiments about the currency based on a sentiment analysis model trained on the UCC corpus and a final prediction model based on the sentiment model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51335b",
   "metadata": {},
   "source": [
    "## The following are the steps needed to access the data files form the source and should not be repeated unless desired. You can skip to the preprocessing section where the data is presented in a neat format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d40e17",
   "metadata": {},
   "source": [
    "To begin, we must first install some modules required to access the data, as per the publisher (Banda et al., 2021):\n",
    "- Twarc\n",
    "- Tweepy (v. 3.8.0)\n",
    "- Argparse (v 3.2)\n",
    "- Xtract (v 0.1 a3)\n",
    "- Wget (v 3.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150a1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install twarc \n",
    "!pip install tweepy \n",
    "!pip install argparse \n",
    "!pip install xtract \n",
    "!pip install ipywidgets\n",
    "!pip install wget\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb4020",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before we run the analysis, we need to filter and process our data to extract only the English tweets.\n",
    "To acces the data, I use the instructions provided by Banda et al from the website: https://github.com/thepanacealab/covid19_twitter/blob/master/COVID_19_dataset_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141a1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import csv\n",
    "import wget\n",
    "import linecache\n",
    "from shutil import copyfile\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfc59c",
   "metadata": {},
   "source": [
    "## Filtering dataset by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed88865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcca2fcd7a744fec90e33581a425c4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=14, options=('all', 'am', 'ar', 'bg', 'bn', 'bo', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'eâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unzips the dataset and gets the TSV dataset\n",
    "with gzip.open('clean-dataset.tsv.gz', 'rb') as f_in:\n",
    "    with open('clean-dataset.tsv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "\n",
    "#Gets all possible languages from the dataset\n",
    "df = pd.read_csv('clean-dataset.tsv',sep=\"\\t\")\n",
    "lang_list = df.lang.unique()\n",
    "lang_list= sorted(np.append(lang_list,'all'))\n",
    "lang_picker = widgets.Dropdown(options=lang_list, value=\"en\")\n",
    "lang_picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7731e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShowing first 5 tweets from the filtered dataset\u001b[0m\n",
      "['1351757442653294592\\t2021-01-20\\t05:04:23\\ten\\tNULL\\n', '1351757444033069056\\t2021-01-20\\t05:04:23\\ten\\tNULL\\n', '1351757446860083202\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n', '1351757447619375106\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n', '1351757448219140105\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n']\n"
     ]
    }
   ],
   "source": [
    "#Creates a new clean dataset with the english tweets and prints out the first 5 twits from the filtered dataset.\n",
    "filtered_language = lang_picker.value\n",
    "\n",
    "#If no language specified, it will get all records from the dataset\n",
    "if filtered_language == \"\":\n",
    "    copyfile('clean-dataset.tsv', 'clean-dataset-filtered.tsv')\n",
    "\n",
    "#If language specified, it will create another tsv file with the filtered records\n",
    "else:\n",
    "    filtered_tw = list()\n",
    "    current_line = 1\n",
    "    with open(\"clean-dataset.tsv\") as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        if current_line == 1:\n",
    "            filtered_tw.append(linecache.getline(\"clean-dataset.tsv\", current_line))\n",
    "\n",
    "            for line in tsvreader:\n",
    "                if line[3] == filtered_language:\n",
    "                      filtered_tw.append(linecache.getline(\"clean-dataset.tsv\", current_line))\n",
    "                current_line += 1\n",
    "\n",
    "print('\\033[1mShowing first 5 tweets from the filtered dataset\\033[0m')\n",
    "print(filtered_tw[1:(6 if len(filtered_tw) > 6 else len(filtered_tw))])\n",
    "\n",
    "with open('clean-dataset-filtered.tsv', 'w') as f_output:\n",
    "    for item in filtered_tw:\n",
    "        f_output.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0d256",
   "metadata": {},
   "source": [
    "## Authentication \n",
    "Accessing the Twitter APIs requires a set of credentials that you must pass with each request. These credentials can come in different forms depending on the type of authentication that is required by the specific endpoint that you are using. More information: https://developer.twitter.com/en/docs/apps/overview\n",
    "\n",
    "The credentials can be obtained from the developer portal (https://developer.twitter.com/en/portal/dashboard).\n",
    "**Ensure that you have an elevated account as the basic essential credentials will not allow you to access the data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8c138bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Authenticate\n",
    "CONSUMER_KEY = \"Api_Key\" #@param {type:\"string\"}\n",
    "CONSUMER_SECRET_KEY = \"Api_secret_Key\" #@param {type:\"string\"}\n",
    "ACCESS_TOKEN_KEY = \"Access_key\" #@param {type:\"string\"}\n",
    "ACCESS_TOKEN_SECRET_KEY = \"Access_secret_Key\" #@param {type:\"string\"}\n",
    "\n",
    "#Creates a JSON Files with the API credentials\n",
    "with open('api_keys.json', 'w') as outfile:\n",
    "    json.dump({\n",
    "    \"consumer_key\":CONSUMER_KEY,\n",
    "    \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
    "    \"access_token\":ACCESS_TOKEN_KEY,\n",
    "    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY\n",
    "     }, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cc27c",
   "metadata": {},
   "source": [
    "## Hydrating the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596337fa",
   "metadata": {},
   "source": [
    "Before parsing the dataset, a hydration process is required.It is done by using the following social media mining tool: https://github.com/thepanacealab/SMMT\n",
    "\n",
    "To perform this action, a python file from that repository is required (get_metadata.py).\n",
    "Once obtained, this utility will take a file which meets the following requirements:\n",
    "\n",
    "- A csv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
    "- A text file which contains one tweet id per line\n",
    "- A tsv file which either contains one tweet id per line or contains at least one column of tweet ids\n",
    "- For this case, the filtered dataset generated before (clean-dataset-filtered.tsv), which is in TSV format will be used for the hydration process\n",
    "\n",
    "The arguments for this utily (get_metadata.py) are the following:\n",
    "- i :input file name\n",
    "- 0 :output file name\n",
    "- k :key file name #json file containing your Api keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_metadata.py -i clean-dataset-filtered.tsv -o hydrated_tweets1 -k api_keys.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb437ed",
   "metadata": {},
   "source": [
    "From the code above, the output will generate four files:\n",
    "\n",
    "- A hydrated_tweets.json file which contains the full json object for each of the hydrated tweets\n",
    "- A hydrated_tweets.CSV file which contains partial fields extracted from the tweets.\n",
    "- A hydrated_tweets.zip file which contains a zipped version of the tweets_full.json file.\n",
    "- A hydrated_tweets_short.json which contains a shortened version of the hydrated tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208c69a",
   "metadata": {},
   "source": [
    "## Parsing Tweets\n",
    "Now to Parse the Tweets we need the following files from the data processing tools:\n",
    "- https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/parse_json_lite.py\n",
    "- https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/fields.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e70a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install emot --upgrade\n",
    "!pip install emoji --upgrade\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67396e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"hydrated_tweets_short.json\", \"r\") as myfile:\n",
    "    list_tweets = list(myfile)\n",
    "    \n",
    "file = open(\"sample_data.json\", \"w\")\n",
    "for i in list_tweets:\n",
    "    file.write(i)\n",
    "file.close() #This close() is important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136c1d3",
   "metadata": {},
   "source": [
    "### The following code uses the utility above to parse the data and preprocess it.\n",
    "\n",
    "parse_json_lite.py: The first argument is the json file. The second argument is optional. If the second argument is given, it will preprocess the json file. The preprocessing includes removal of URLs, twitter specific Urls, Emojis, Emoticons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592d6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python parse_json_lite.py sample_data.json p\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8f8ed",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "Now that the data is Hydrated and organised in the four files, we can begin the preprocessing of the data.\n",
    "\n",
    "> For our project, we perform a sentiment analysis on tweets related to crypto currencies and use this analysis to predict how the currencies will varry depending on the sentiment. \n",
    "\n",
    "> Since we are only interested in tweets that are related to Bitcoin, we will specify a filter then filter out the tweets that do not contain the words in the filter.\n",
    "\n",
    ">After that, we perform a sentiment analysis using pre trained models to see whether we can accurately predict what the sentiment of the tweets are.\n",
    "\n",
    ">The models used will be trained on the UCC(The Unhealthy Comments Corpus) Coprus that was mentioned before , which contains over 40,000 online comments which have been tagged with sentiment values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45e7b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "03acf5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_json('sample_data.json', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5542791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-20 05:04:27+00:00</td>\n",
       "      <td>Biden leads observance of America's 400,000 CO...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1351757459166285824</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-20 05:04:49+00:00</td>\n",
       "      <td>WORKING smoke alarms save lives!  Check yours ...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1351757551814258688</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-20 05:04:31+00:00</td>\n",
       "      <td>24 million total U.S. Covid-19 infections were...</td>\n",
       "      <td>MillikenReports</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1351757476123668480</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-20 05:05:12+00:00</td>\n",
       "      <td>SIR Simulation of COVID-19 Pandemic in Malaysi...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twibble.io</td>\n",
       "      <td>1351757647461150720</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-20 05:04:47+00:00</td>\n",
       "      <td>Expect changes to gatherings, personal service...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Echobox</td>\n",
       "      <td>1351757544566489088</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0 2021-01-20 05:04:27+00:00   \n",
       "1 2021-01-20 05:04:49+00:00   \n",
       "2 2021-01-20 05:04:31+00:00   \n",
       "3 2021-01-20 05:05:12+00:00   \n",
       "4 2021-01-20 05:04:47+00:00   \n",
       "\n",
       "                                                text in_reply_to_screen_name  \\\n",
       "0  Biden leads observance of America's 400,000 CO...                    None   \n",
       "1  WORKING smoke alarms save lives!  Check yours ...                    None   \n",
       "2  24 million total U.S. Covid-19 infections were...         MillikenReports   \n",
       "3  SIR Simulation of COVID-19 Pandemic in Malaysi...                    None   \n",
       "4  Expect changes to gatherings, personal service...                    None   \n",
       "\n",
       "   retweet_count  favorite_count              source               id_str  \\\n",
       "0              0               0     Twitter Web App  1351757459166285824   \n",
       "1              1               0     Twitter Web App  1351757551814258688   \n",
       "2              0               0  Twitter for iPhone  1351757476123668480   \n",
       "3              0               0          Twibble.io  1351757647461150720   \n",
       "4              0               0             Echobox  1351757544566489088   \n",
       "\n",
       "   is_retweet  \n",
       "0       False  \n",
       "1       False  \n",
       "2       False  \n",
       "3       False  \n",
       "4       False  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67e5754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'text', 'in_reply_to_screen_name', 'retweet_count', 'favorite_count', 'source', 'id_str', 'is_retweet']\n"
     ]
    }
   ],
   "source": [
    "heads = list(df.head().columns.values.tolist())\n",
    "print(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9cb1e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['created_at', 'in_reply_to_screen_name', 'retweet_count', 'favorite_count', 'source', 'id_str'], axis=1, inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "abb4fde5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden leads observance of America's 400,000 CO...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WORKING smoke alarms save lives!  Check yours ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24 million total U.S. Covid-19 infections were...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SIR Simulation of COVID-19 Pandemic in Malaysi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expect changes to gatherings, personal service...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_retweet\n",
       "0  Biden leads observance of America's 400,000 CO...       False\n",
       "1  WORKING smoke alarms save lives!  Check yours ...       False\n",
       "2  24 million total U.S. Covid-19 infections were...       False\n",
       "3  SIR Simulation of COVID-19 Pandemic in Malaysi...       False\n",
       "4  Expect changes to gatherings, personal service...       False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eea40b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden leads observance of America's 400,000 CO...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WORKING smoke alarms save lives!  Check yours ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24 million total U.S. Covid-19 infections were...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SIR Simulation of COVID-19 Pandemic in Malaysi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expect changes to gatherings, personal service...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_retweet\n",
       "0  Biden leads observance of America's 400,000 CO...       False\n",
       "1  WORKING smoke alarms save lives!  Check yours ...       False\n",
       "2  24 million total U.S. Covid-19 infections were...       False\n",
       "3  SIR Simulation of COVID-19 Pandemic in Malaysi...       False\n",
       "4  Expect changes to gatherings, personal service...       False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=[False]\n",
    "data= df[df.is_retweet.isin(lst)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56f51d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_tweets = []           #List of Tokenized Tweets \n",
    "\n",
    "for Tweet, is_retweet in zip(df.text, df.is_retweet):\n",
    "    if is_retweet == False:\n",
    "        tokenized_tweet = TweetTokenizer().tokenize(Tweet)\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "\n",
    "#store the list with respective pos tag\n",
    "tagged_tweets = [nltk.pos_tag(tweet, tagset = \"universal\") for tweet in tokenized_tweets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b419a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_urls = re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\n",
    "remove_html_tags = re.compile(r'<[^>]+>') #Removing non-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d980e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_tweets= []   #Final list of filtered tweets\n",
    "\n",
    "for tweet in tokenized_tweets:\n",
    "    lemmatized_tweet = []\n",
    "    for word in tweet:\n",
    "        word.lower()\n",
    "        word = remove_urls.sub(' ', word)\n",
    "        word = remove_html_tags.sub('', word)\n",
    "        \n",
    "        # I choose only to keep words that are longer than 3 chars\n",
    "        # in my lemmatization\n",
    "        \n",
    "        if ((len(word) > 3) and (not word in stopwords)): \n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(word)\n",
    "            lemmatized_tweet.append(lemma)\n",
    "    if lemmatized_tweet != []:\n",
    "        clean_tweet = \" \".join(lemmatized_tweet)\n",
    "        filtered_tweets.append(clean_tweet)\n",
    "\n",
    "#formatting the data back into a data frame\n",
    "\n",
    "df_formatted = pd.DataFrame(filtered_tweets, columns =['Formatted_tweets']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e424b6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formatted_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden lead observance America's 400,000 COVID ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WORKING smoke alarm save life Check today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>million total Covid infection reported Monday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simulation COVID Pandemic Malaysia Will Vaccin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expect change gathering personal service shopp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Formatted_tweets\n",
       "0  Biden lead observance America's 400,000 COVID ...\n",
       "1          WORKING smoke alarm save life Check today\n",
       "2  million total Covid infection reported Monday ...\n",
       "3  Simulation COVID Pandemic Malaysia Will Vaccin...\n",
       "4  Expect change gathering personal service shopp..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_formatted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2df9d7",
   "metadata": {},
   "source": [
    "## Apple-Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucc_train = pd.read_csv(\"UCC/train.csv\")\n",
    "aapl = pd.read_csv(\"data/Apple-Twitter-Sent.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95de0e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623495513</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>#AAPL:The 10 best Steve Jobs emails ever...htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>623495514</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8129</td>\n",
       "      <td>RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>623495515</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>My cat only chews @apple cords. Such an #Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623495516</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>I agree with @jimcramer that the #IndividualIn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>623495517</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6474</td>\n",
       "      <td>Nobody expects the Spanish Inquisition #AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>623499442</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>(Via FC) Apple Is Warming Up To Social Media -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>623499450</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>RT @MMLXIV: there is no avocado emoji may I as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>623499486</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9347</td>\n",
       "      <td>@marcbulandr I could not agree more. Between @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>623499514</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>My iPhone 5's photos are no longer downloading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>623517290</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8938</td>\n",
       "      <td>RT @SwiftKey: We're so excited to be named to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3886 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _trusted_judgments sentiment  sentiment:confidence  \\\n",
       "0     623495513                  10         3                0.6264   \n",
       "1     623495514                  12         3                0.8129   \n",
       "2     623495515                  10         3                1.0000   \n",
       "3     623495516                  17         3                0.5848   \n",
       "4     623495517                   3         3                0.6474   \n",
       "...         ...                 ...       ...                   ...   \n",
       "3881  623499442                  13         3                0.7757   \n",
       "3882  623499450                  16         3                0.6225   \n",
       "3883  623499486                  14         5                0.9347   \n",
       "3884  623499514                  13         1                0.9230   \n",
       "3885  623517290                  17         5                0.8938   \n",
       "\n",
       "                                                   text  \n",
       "0     #AAPL:The 10 best Steve Jobs emails ever...htt...  \n",
       "1     RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...  \n",
       "2     My cat only chews @apple cords. Such an #Apple...  \n",
       "3     I agree with @jimcramer that the #IndividualIn...  \n",
       "4          Nobody expects the Spanish Inquisition #AAPL  \n",
       "...                                                 ...  \n",
       "3881  (Via FC) Apple Is Warming Up To Social Media -...  \n",
       "3882  RT @MMLXIV: there is no avocado emoji may I as...  \n",
       "3883  @marcbulandr I could not agree more. Between @...  \n",
       "3884  My iPhone 5's photos are no longer downloading...  \n",
       "3885  RT @SwiftKey: We're so excited to be named to ...  \n",
       "\n",
       "[3886 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl = aapl.drop(columns=\"_golden _unit_state _last_judgment_at date id query sentiment_gold\".split())\n",
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dae8032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('My', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('chews', 'VBZ'),\n",
       "  ('@apple', 'JJ'),\n",
       "  ('cords', 'NNS'),\n",
       "  ('Such', 'JJ'),\n",
       "  ('an', 'DT'),\n",
       "  ('#AppleSnob', 'NN')],\n",
       " '3')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "tweets = list(zip(aapl[\"text\"], aapl[\"sentiment\"]))\n",
    "\n",
    "ttk = TweetTokenizer()\n",
    "\n",
    "tokens = [(ttk.tokenize(tweet), sentiment) for tweet, sentiment in tweets]\n",
    "\n",
    "filtered = []\n",
    "for tweet in tokens:\n",
    "    toks = []\n",
    "    for tok in tweet[0]:\n",
    "        if tok.isalpha():\n",
    "            toks.append(tok)\n",
    "        if \"#\" in tok or \"@\" in tok:\n",
    "            toks.append(tok)\n",
    "    filtered.append((toks, tweet[1]))\n",
    "\n",
    "tagged = [(nltk.pos_tag(tweet), sentiment) for tweet, sentiment in filtered]\n",
    "\n",
    "tagged[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
