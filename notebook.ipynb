{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8031b7fc",
   "metadata": {},
   "source": [
    "# Tweet_Sentiment_Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed56d7b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For our project, we have chosen to use twitter sentiment data about crypto currencies to create a predictive model that relies on the sentiment to predict how different crpyto currencies may behave. \n",
    "We have chosen the following datasets for our analysis:\n",
    "- Covid-19 Twitter chatter dataset\n",
    "- \n",
    "\n",
    "\n",
    "To begin, we must first install some modules required to access the data, as per the publisher (Banda et al., 2021):\n",
    "- Twarc\n",
    "- Tweepy (v. 3.8.0)\n",
    "- Argparse (v 3.2)\n",
    "- Xtract (v 0.1 a3)\n",
    "- Wget (v 3.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "150a1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install twarc \n",
    "!pip install tweepy \n",
    "!pip install argparse \n",
    "!pip install xtract \n",
    "!pip install ipywidgets\n",
    "!pip install wget\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a414353",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before we run the analysis, we need to filter and process our data to extract only the English tweets.\n",
    "To acces the data, I use the instructions provided by Banda et al from the website: https://github.com/thepanacealab/covid19_twitter/blob/master/COVID_19_dataset_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "141a1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import csv\n",
    "import wget\n",
    "import linecache\n",
    "from shutil import copyfile\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3a0e3",
   "metadata": {},
   "source": [
    "## Filtering dataset by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4e78a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6259a648360a40cc871747cbd277fb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=14, options=('all', 'am', 'ar', 'bg', 'bn', 'bo', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'dv', 'eâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unzips the dataset and gets the TSV dataset\n",
    "with gzip.open('clean-dataset.tsv.gz', 'rb') as f_in:\n",
    "    with open('clean-dataset.tsv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "#Deletes the compressed GZ file\n",
    "os.unlink(\"clean-dataset.tsv.gz\")\n",
    "\n",
    "#Gets all possible languages from the dataset\n",
    "df = pd.read_csv('clean-dataset.tsv',sep=\"\\t\")\n",
    "lang_list = df.lang.unique()\n",
    "lang_list= sorted(np.append(lang_list,'all'))\n",
    "lang_picker = widgets.Dropdown(options=lang_list, value=\"en\")\n",
    "lang_picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50b52027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShowing first 5 tweets from the filtered dataset\u001b[0m\n",
      "['tweet_id\\tdate\\ttime\\tlang\\tcountry_code\\n', '1351757442653294592\\t2021-01-20\\t05:04:23\\ten\\tNULL\\n', '1351757444033069056\\t2021-01-20\\t05:04:23\\ten\\tNULL\\n', '1351757446860083202\\t2021-01-20\\t05:04:24\\ten\\tNULL\\n', '1351757447438991366\\t2021-01-20\\t05:04:24\\tur\\tNULL\\n']\n"
     ]
    }
   ],
   "source": [
    "#Creates a new clean dataset with the english tweets and prints out the first 5 twits from the filtered dataset.\n",
    "filtered_language = lang_picker.value\n",
    "filtered_tw = list()\n",
    "current_line = 1\n",
    "with open(\"clean-dataset.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    if current_line == 1:\n",
    "        filtered_tw.append(linecache.getline(\"clean-dataset.tsv\", current_line))\n",
    "        \n",
    "        for line in tsvreader:\n",
    "            if line[3] == filtered_language:\n",
    "                filtered_tw.append(linecache.getline(\"clean-dataset.tsv\", current_line))\n",
    "                current_line += 1\n",
    "\n",
    "print('\\033[1mShowing first 5 tweets from the filtered dataset\\033[0m')\n",
    "print(filtered_tw[1:(6 if len(filtered_tw) > 6 else len(filtered_tw))])\n",
    "\n",
    "with open('clean-dataset-filtered.tsv', 'w') as f_output:\n",
    "    for item in filtered_tw:\n",
    "        f_output.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2310d82",
   "metadata": {},
   "source": [
    "## Authentication \n",
    "Accessing the Twitter APIs requires a set of credentials that you must pass with each request. These credentials can come in different forms depending on the type of authentication that is required by the specific endpoint that you are using. More information: https://developer.twitter.com/en/docs/apps/overview\n",
    "\n",
    "The credentials can be obtained from the developer portal (https://developer.twitter.com/en/portal/dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd26dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# Authenticate\n",
    "CONSUMER_KEY = \"bUahq0OHF0HHbqlzSDOqu7WxB\" #@param {type:\"string\"}\n",
    "CONSUMER_SECRET_KEY = \"rPBz1gh5exTMLh9qE2wjZRprjID8q9gRyPj6K2GLCTb1PRd6RE\" #@param {type:\"string\"}\n",
    "ACCESS_TOKEN_KEY = \"1069349234-MeWx5SCqoqnk8NyzOD6sRfO0g5oSJOYKfjYLxrV\" #@param {type:\"string\"}\n",
    "ACCESS_TOKEN_SECRET_KEY = \"8ixQqIgfUEmMxswxhB5bSVrUp9Uwe8z3mbPW8ZUGMWzVy\" #@param {type:\"string\"}\n",
    "\n",
    "#Creates a JSON Files with the API credentials\n",
    "with open('api_keys.json', 'w') as outfile:\n",
    "    json.dump({\n",
    "    \"consumer_key\":CONSUMER_KEY,\n",
    "    \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
    "    \"access_token\":ACCESS_TOKEN_KEY,\n",
    "    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY\n",
    "     }, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4988a4",
   "metadata": {},
   "source": [
    "## Hydrating the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d14b6b",
   "metadata": {},
   "source": [
    "Before parsing the dataset, an hydration process is required. In this tutorial it is done by using the following social media mining tool: https://github.com/thepanacealab/SMMT\n",
    "\n",
    "To perform this action, a python file from that repository is required (get_metadata.py) and can be obtained in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ea93780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_acquisition/get_metadata.py -O get_metadata.py\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac75448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_metadata.py -i clean-dataset-filtered.tsv -o hydrated_tweets -k api_keys.json\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d057537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/parse_json_lite.py -O parse_json_lite.py\n",
    "!wget https://raw.githubusercontent.com/thepanacealab/SMMT/master/data_preprocessing/fields.py -O fields.py\n",
    "\n",
    "!pip install emot --upgrade\n",
    "!pip install emoji --upgrade\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d87da7",
   "metadata": {},
   "source": [
    "## Pulling a sample set from the tweets\n",
    "Here we try to display the first five tweets after hydration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cc4b18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hydrated_tweets_short.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhydrated_tweets_short.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m myfile:\n\u001b[0;32m      2\u001b[0m     list_tweets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(myfile)\n\u001b[0;32m      3\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hydrated_tweets_short.json'"
     ]
    }
   ],
   "source": [
    "with open(\"hydrated_tweets_short.json\", \"r\") as myfile:\n",
    "    list_tweets = list(myfile)\n",
    "n = 5 \n",
    "for tweet in range(n):\n",
    "    print(list_tweets[tweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba12a5a",
   "metadata": {},
   "source": [
    "## Apple-Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a1422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucc_train = pd.read_csv(\"UCC/train.csv\")\n",
    "aapl = pd.read_csv(\"data/Apple-Twitter-Sent.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95de0e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623495513</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>#AAPL:The 10 best Steve Jobs emails ever...htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>623495514</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8129</td>\n",
       "      <td>RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>623495515</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>My cat only chews @apple cords. Such an #Apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623495516</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>I agree with @jimcramer that the #IndividualIn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>623495517</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6474</td>\n",
       "      <td>Nobody expects the Spanish Inquisition #AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>623499442</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>(Via FC) Apple Is Warming Up To Social Media -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>623499450</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6225</td>\n",
       "      <td>RT @MMLXIV: there is no avocado emoji may I as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>623499486</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9347</td>\n",
       "      <td>@marcbulandr I could not agree more. Between @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>623499514</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>My iPhone 5's photos are no longer downloading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>623517290</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8938</td>\n",
       "      <td>RT @SwiftKey: We're so excited to be named to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3886 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _trusted_judgments sentiment  sentiment:confidence  \\\n",
       "0     623495513                  10         3                0.6264   \n",
       "1     623495514                  12         3                0.8129   \n",
       "2     623495515                  10         3                1.0000   \n",
       "3     623495516                  17         3                0.5848   \n",
       "4     623495517                   3         3                0.6474   \n",
       "...         ...                 ...       ...                   ...   \n",
       "3881  623499442                  13         3                0.7757   \n",
       "3882  623499450                  16         3                0.6225   \n",
       "3883  623499486                  14         5                0.9347   \n",
       "3884  623499514                  13         1                0.9230   \n",
       "3885  623517290                  17         5                0.8938   \n",
       "\n",
       "                                                   text  \n",
       "0     #AAPL:The 10 best Steve Jobs emails ever...htt...  \n",
       "1     RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...  \n",
       "2     My cat only chews @apple cords. Such an #Apple...  \n",
       "3     I agree with @jimcramer that the #IndividualIn...  \n",
       "4          Nobody expects the Spanish Inquisition #AAPL  \n",
       "...                                                 ...  \n",
       "3881  (Via FC) Apple Is Warming Up To Social Media -...  \n",
       "3882  RT @MMLXIV: there is no avocado emoji may I as...  \n",
       "3883  @marcbulandr I could not agree more. Between @...  \n",
       "3884  My iPhone 5's photos are no longer downloading...  \n",
       "3885  RT @SwiftKey: We're so excited to be named to ...  \n",
       "\n",
       "[3886 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl = aapl.drop(columns=\"_golden _unit_state _last_judgment_at date id query sentiment_gold\".split())\n",
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dae8032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('My', 'PRP$'),\n",
       "  ('cat', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('chews', 'VBZ'),\n",
       "  ('@apple', 'JJ'),\n",
       "  ('cords', 'NNS'),\n",
       "  ('Such', 'JJ'),\n",
       "  ('an', 'DT'),\n",
       "  ('#AppleSnob', 'NN')],\n",
       " '3')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "tweets = list(zip(aapl[\"text\"], aapl[\"sentiment\"]))\n",
    "\n",
    "ttk = TweetTokenizer()\n",
    "\n",
    "tokens = [(ttk.tokenize(tweet), sentiment) for tweet, sentiment in tweets]\n",
    "\n",
    "filtered = []\n",
    "for tweet in tokens:\n",
    "    toks = []\n",
    "    for tok in tweet[0]:\n",
    "        if tok.isalpha():\n",
    "            toks.append(tok)\n",
    "        if \"#\" in tok or \"@\" in tok:\n",
    "            toks.append(tok)\n",
    "    filtered.append((toks, tweet[1]))\n",
    "\n",
    "tagged = [(nltk.pos_tag(tweet), sentiment) for tweet, sentiment in filtered]\n",
    "\n",
    "tagged[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
